{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "### TODO: In this cell, write an explanation of which dataset you have chosen and why it is appropriate for this task\n",
    "\n",
    "In this Chatbot project I will be collecting data from Wikipedia page of 2024. This dataset I have chosen is appropriate for this task since  I have two question from events on 2024 and the model of gpt-35-turbo was updated till 2021.\n",
    "\n",
    "We can provide context (RAG) to th model for obtening the response from 2024.\n",
    "\n",
    "Question 1: \"Who was elected president of the United States of America?\"\n",
    "Question 2: \"Which country won the Olympic Games of Paris 2024?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f631a9",
   "metadata": {},
   "source": [
    "# Step 0 - Initial OpenAI response\n",
    "\n",
    "Before I custom a chatbot, I want to ask the openAI model 2 questions from 2024 to check its answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89169298",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "import pandas as pd\n",
    "import requests\n",
    "import openai\n",
    "from openai.embeddings_utils import get_embedding, distances_from_embeddings\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "\n",
    "openai.api_base = \"https://openai.vocareum.com/v1\"\n",
    "openai.api_key = \"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b95fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_answer(question):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=question,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb246779",
   "metadata": {},
   "source": [
    "The model is answering this way because the training data ends in 2021. My task will be to provide context from 2024 to help the model answer these questions correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "# STEP 01 - DATA INDEXING\n",
    "\n",
    "TODO: In the cells below, load your chosen dataset into a `pandas` dataframe with a column named `\"text\"`. This column should contain all of your text data, separated into at least 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69b83a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_wikipedia(title):\n",
    "    #Getting the wikipedia page for 2024\n",
    "    params = {\n",
    "       \"action\": \"query\", \n",
    "       \"prop\": \"extracts\",\n",
    "       \"exlimit\": 1,\n",
    "       \"titles\": title,\n",
    "       \"explaintext\": 1,\n",
    "       \"formatversion\": 2,\n",
    "       \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(\"https://en.wikipedia.org/w/api.php\", params)\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df[\"text\"] = response.json()[\"query\"][\"pages\"][0][\"extract\"].split(\"\\n\")\n",
    "    \n",
    "    # Cleaning text to remove empty lines\n",
    "    df = df[(df[\"text\"].str.len() > 0) & (~df[\"text\"].str.startswith(\"==\"))]\n",
    "   \n",
    "    # adjusting so dataset starts with dates (if possible)\n",
    "    prefix = \"\"\n",
    "    for (i, row) in df.iterrows():\n",
    "        # If the row already has \" - \", it already has the needed date prefix\n",
    "        if \" – \" not in row[\"text\"]:\n",
    "            try:\n",
    "                # If the row's text is a date, set it as the new prefix\n",
    "                parse(row[\"text\"])\n",
    "                prefix = row[\"text\"]\n",
    "            except:\n",
    "                # If the row's text isn't a date, add the prefix\n",
    "                row[\"text\"] = prefix + \" – \" + row[\"text\"]\n",
    "    df = df[df[\"text\"].str.contains(\" – \")]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244d7a93",
   "metadata": {},
   "source": [
    "## Creating an Embeddings Index\n",
    "\n",
    "Using \"text-embedding-ada-002\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "babea3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(df, embedding_model_name=\"text-embedding-ada-002\", batch_size=100):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        # Send text data to OpenAI model to get embeddings\n",
    "        response = openai.Embedding.create(\n",
    "            input=df.iloc[i:i+batch_size][\"text\"].tolist(),\n",
    "            engine=embedding_model_name\n",
    "        )\n",
    "\n",
    "        # Add embeddings to list\n",
    "        embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n",
    "\n",
    "    # Add embeddings list to dataframe\n",
    "    df[\"embeddings\"] = embeddings\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24443b2f",
   "metadata": {},
   "source": [
    "## Saving the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e82343d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(df, output_file):\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "#save_embeddings(df, \"embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c1454b",
   "metadata": {},
   "source": [
    "## Load embeddings for cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3a41934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path):\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "    df[\"embeddings\"] = df[\"embeddings\"].apply(eval).apply(np.array)\n",
    "    return df\n",
    "\n",
    "#load_embeddings(\"embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c63488",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "I want to look for the closest distance to compare the vectors from my question and the dataset from WIKIPEDIA 2024 with the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b063ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(question, df, embedding_model_name=\"text-embedding-ada-002\"):\n",
    "    question_embeddings = get_embedding(question, engine=embedding_model_name)\n",
    "    df_copy = df.copy()\n",
    "    df_copy[\"distances\"] = distances_from_embeddings(\n",
    "        question_embeddings,\n",
    "        df_copy[\"embeddings\"].values,\n",
    "        distance_metric=\"cosine\"\n",
    "    )\n",
    "    \n",
    "    # Sort the copied dataframe by the distances and return it\n",
    "    df_copy.sort_values(\"distances\", ascending=True, inplace=True)\n",
    "    return df_copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae769871",
   "metadata": {},
   "source": [
    "# STEP 2 - Custom Query Completion\n",
    "\n",
    "TODO: In the cells below, compose a custom query using your chosen dataset and retrieve results from an OpenAI `Completion` model. You may copy and paste any useful code from the course materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967b6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(question, df, max_token_count=1000):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    prompt_template = \"\"\"\n",
    "    Answer the question based on the context below, and if the question\n",
    "    can't be answered based on the context, say \"I don't know\"\n",
    "\n",
    "    Context: \n",
    "\n",
    "    {}\n",
    "\n",
    "    ---\n",
    "\n",
    "    Question: {}\n",
    "    Answer:\"\"\"   \n",
    "    \n",
    "    current_token_count = len(tokenizer.encode(prompt_template)) + len(tokenizer.encode(question))\n",
    "    context = []\n",
    "    \n",
    "    for text in df[\"text\"].values:\n",
    "        text_token_count = len(tokenizer.encode(text))\n",
    "\n",
    "        if current_token_count <= max_token_count:\n",
    "            context.append(text)\n",
    "            current_token_count += text_token_count\n",
    "\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return prompt_template.format(\"\\n\\n###\\n\\n\".join(context), question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783f146",
   "metadata": {},
   "source": [
    "# STEP 06 Custom Performance Demonstration\n",
    "\n",
    "TODO: In the cells below, demonstrate the performance of your custom query using at least 2 questions. For each question, show the answer from a basic `Completion` model query as well as the answer from your custom query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3d4d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openai_answer(question, max_answer_count):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=question,\n",
    "        max_tokens=max_answer_count\n",
    "    )\n",
    "    return response[\"choices\"][0][\"text\"].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a68d8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_answer(question, df, max_prompt_count=1000, max_answer_count=200):\n",
    "    \n",
    "    #Cosine similarity with most relevant\n",
    "    relevant_rows = cosine_similarity(question, df)\n",
    "    \n",
    "    #prompt for the question and with relevant rows to answer\n",
    "    prompt=create_prompt(question, relevant_rows, max_token_count=max_prompt_count)\n",
    "    \n",
    "    #\n",
    "    return openai_answer(prompt, max_answer_count=max_answer_count)\n",
    "\n",
    "#context_answer(\"Who was elected president of the United States of America?\", df, max_prompt_count=1000, max_answer_count=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8b809",
   "metadata": {},
   "source": [
    "## Customize Chatbot for wikipedia 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dee262a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_demostration(user_input, df):\n",
    "        \n",
    "    #Embeddings, save and load\n",
    "    df_context = generate_embeddings(df)\n",
    "    #save_embeddings(df, \"embeddings.csv\")\n",
    "    #load_embeddings(\"embeddings.csv\")\n",
    "    \n",
    "    #get question from user\n",
    "    question = user_input\n",
    "    \n",
    "    #answer before providing context\n",
    "    before_answer = initial_answer(question)\n",
    "    \n",
    "    #answer after providing context\n",
    "    answer_context = context_answer(question, df_context)\n",
    "    \n",
    "    #Show the resutls\n",
    "    print(f\"\\n\\nThe answer for your question: {question} before providing context to openai is: {before_answer}\\n\\n\")\n",
    "    print(f\"After we provided context, the answer is: {answer_context}\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86e37c",
   "metadata": {},
   "source": [
    "### Chatbot that receives input from the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7467f584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to know from 2024 events?:  \n",
      "\n",
      "Who was elected president of the United States of America? \n",
      "\n",
      "\n",
      "The answer for your question: who was elected president of the united states of america?  before providing context to openai is: As of May 2021, Joe Biden is the elected President of the United States of America.\n",
      "\n",
      "\n",
      "After we provided context, the answer is: Donald Trump\n",
      "\n",
      "\n",
      "What do you want to know from 2024 events?:  \n",
      "\n",
      "When was X (former twitter) banned in Brasil?\n",
      "\n",
      "\n",
      "The answer for your question: when was x (former twitter) banned in brasil? before providing context to openai is: Former Twitter was not banned in Brazil. Twitter itself has not been banned in Brazil. However, there have been instances where certain features or accounts on Twitter have been temporarily blocked in Brazil due to court orders.\n",
      "\n",
      "\n",
      "After we provided context, the answer is: September 2\n",
      "\n",
      "\n",
      "You run out of questions for today, try again tomorrow!\n"
     ]
    }
   ],
   "source": [
    "chatbot_on = True\n",
    "user_questions = 0\n",
    "\n",
    "#loading dataset from wikipedia 2024\n",
    "df = load_dataset_wikipedia(\"2024\")\n",
    "    \n",
    "while chatbot_on:\n",
    "    user_input = input(\"What do you want to know from 2024 events?:  \\n\\n\").lower()\n",
    "    main_demostration(user_input, df)\n",
    "    user_questions += 1\n",
    "    \n",
    "    if user_questions ==2:\n",
    "        chatbot_on = False\n",
    "        print(\"You run out of questions for today, try again tomorrow!\")\n",
    "\n",
    "#Question 1: Who was elected president of the United States of America? \n",
    "#Question 2: When was X (former twitter) banned in Brasil?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
